# main.py
from crawler_appium import IntegratedCrawler
from analyze_report import BuryPointAnalyzer
from time import time
import threading
import os
import json
from config import CONFIG
import requests

print("""
=====================================
 åŸ‹ç‚¹è‡ªåŠ¨åŒ–æ£€æµ‹ Demo
=====================================
""")

print("âš ï¸ è¯·ç¡®è®¤ï¼š")
print("1. mitmdump -s capture_mitm.py å·²å¯åŠ¨ (å¦‚éœ€æŠ“åŒ…)")
print("2. æ‰‹æœºä»£ç†å·²æŒ‡å‘ç”µè„‘ IP:8080 (å¦‚éœ€æŠ“åŒ…)")
print("3. æˆ–è€…æš‚æ—¶ä¸æŠ“åŒ…,åªæµ‹è¯•éå†åŠŸèƒ½")
input("\nç¡®è®¤åå›è½¦å¼€å§‹...")


requests.get(
    "http://mark.local/__start_session__",
    proxies={
        "http": "http://127.0.0.1:8080",
        "https": "http://127.0.0.1:8080",
    },
    timeout=2
)

session_file = "log/current_mitm_session.json"

if os.path.exists(session_file):
    with open(session_file, "r", encoding="utf-8") as f:
        mitm_session = json.load(f)

    print(f"ğŸ§­ å‘ç°æŠ“åŒ…ä¼šè¯:")
    print(f"   session_id = {mitm_session['session_id']}")
    print(f"   log_file   = {mitm_session['log_file']}")
else:
    mitm_session = None
    print("âš ï¸ æœªå‘ç° mitm æŠ“åŒ…ä¼šè¯ï¼Œå°†ä»…åš UI éå†")

crawler = IntegratedCrawler(config = CONFIG,
                            mitm_log_file=mitm_session["log_file"] if mitm_session else None)

try:
    start = time()
    crawler.run()
    print(f"â± éå†è€—æ—¶ {time() - start:.1f}s")
finally:
    print("ğŸ§¹ åå°æ¸…ç† crawler...")
    threading.Thread(
        target=crawler.stop,
        daemon=True
    ).start()

print("\nâœ… éå†å®Œæˆï¼")

print("\nğŸ“Š ç”ŸæˆåŸ‹ç‚¹è¯„ä¼°æŠ¥å‘Š...")
try:
    report_file = BuryPointAnalyzer(mitm_file = f"log/mitm_requests_{mitm_session['session_id']}" , click_log_file = f"log/click_log_{mitm_session['session_id']}").generate_report()
    print(f"âœ… æŠ¥å‘Šä¿å­˜ä¸º: {report_file}")
except Exception as e:
    print(f"âš ï¸ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
